{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 Shades of Text  - Leveraging Natural Language Processing\n",
    "\n",
    "## Notebook explained during the talk I gave in June 2018 available at the following link:\n",
    "## [Video](https://youtu.be/M6U_YrnWIa8?t=224)\n",
    "## Data used for this notebook: \n",
    "## [Dataset](https://www.kaggle.com/c/home-depot-product-search-relevance/data)\n",
    "## [Wikipedia GloVe Word Embeddings](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Input,InputLayer,Reshape\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv1D,Conv2D\n",
    "from keras.layers import GRU, LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gensim\n",
    "import keras\n",
    "import warnings\n",
    "from gensim.models import KeyedVectors\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to generate ngrams given a sentence\n",
    "\n",
    "def word2ngrams(text, ngrams = 4):\n",
    "    return [\"\".join(j) for j in zip(*[text[i:] for i in range(ngrams)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   3       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   9       100002  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...   \n",
       "3  16       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "4  17       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "\n",
       "          search_term  relevance  \n",
       "0       angle bracket       3.00  \n",
       "1           l bracket       2.50  \n",
       "2           deck over       3.00  \n",
       "3    rain shower head       2.33  \n",
       "4  shower only faucet       2.67  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read File training and attributes and extracting the category property\n",
    "\n",
    "np.random.seed(32)\n",
    "#Reading training set\n",
    "df = pd.read_csv('data/train.csv' , sep=',' , encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet01</td>\n",
       "      <td>Versatile connector for various 90Â° connectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet02</td>\n",
       "      <td>Stronger than angled nailing or screw fastenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet03</td>\n",
       "      <td>Help ensure joints are consistently straight a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet04</td>\n",
       "      <td>Dimensions: 3 in. x 3 in. x 1-1/2 in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100001.0</td>\n",
       "      <td>Bullet05</td>\n",
       "      <td>Made from 12-Gauge steel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid      name                                              value\n",
       "0     100001.0  Bullet01  Versatile connector for various 90Â° connectio...\n",
       "1     100001.0  Bullet02  Stronger than angled nailing or screw fastenin...\n",
       "2     100001.0  Bullet03  Help ensure joints are consistently straight a...\n",
       "3     100001.0  Bullet04              Dimensions: 3 in. x 3 in. x 1-1/2 in.\n",
       "4     100001.0  Bullet05                           Made from 12-Gauge steel"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df with attributes\n",
    "\n",
    "df_attr = pd.read_csv('data/attributes.csv' , sep=',' , encoding='latin-1')\n",
    "df_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Count df with label 6169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>100037.0</td>\n",
       "      <td>Tools Product Type</td>\n",
       "      <td>Hand Tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>100038.0</td>\n",
       "      <td>Tools Product Type</td>\n",
       "      <td>Power Tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>100073.0</td>\n",
       "      <td>Tools Product Type</td>\n",
       "      <td>Tool Storage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>100093.0</td>\n",
       "      <td>Tools Product Type</td>\n",
       "      <td>Power Tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>100099.0</td>\n",
       "      <td>Tools Product Type</td>\n",
       "      <td>Power Tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_uid                name         value\n",
       "846      100037.0  Tools Product Type     Hand Tool\n",
       "879      100038.0  Tools Product Type    Power Tool\n",
       "1715     100073.0  Tools Product Type  Tool Storage\n",
       "2322     100093.0  Tools Product Type    Power Tool\n",
       "2526     100099.0  Tools Product Type    Power Tool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the two datasets\n",
    "\n",
    "df_with_label = df_attr[df_attr['name'] == 'Tools Product Type']\n",
    "print('*'*50)\n",
    "print('Count df with label',len(df_with_label))\n",
    "df_with_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  3991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "      <th>value</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>husky tool bag</td>\n",
       "      <td>Hand Tool</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impact driver drill battery powered</td>\n",
       "      <td>Power Tool</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>impact wrench</td>\n",
       "      <td>Power Tool</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milwaukee right angle</td>\n",
       "      <td>Power Tool</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>milwaukee stone hole drill</td>\n",
       "      <td>Power Tool</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           search_term       value  cat\n",
       "0                       husky tool bag   Hand Tool    5\n",
       "1  impact driver drill battery powered  Power Tool    8\n",
       "2                        impact wrench  Power Tool    8\n",
       "3                milwaukee right angle  Power Tool    8\n",
       "4           milwaukee stone hole drill  Power Tool    8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the 2 dataset to create training data\n",
    "\n",
    "joined = pd.merge(df, df_with_label, on='product_uid', how='inner')[['search_term','value']]\n",
    "print('Dataset size: ',len(joined))\n",
    "len(joined.value.unique())\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "joined[\"cat\"] = lb_make.fit_transform(joined[\"value\"])\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of search query: impact driver drill battery powered\n",
      "Example of ngrams:  ['impa', 'mpac', 'pact', 'act ', 'ct d', 't dr', ' dri', 'driv', 'rive', 'iver', 'ver ', 'er d', 'r dr', ' dri', 'dril', 'rill', 'ill ', 'll b', 'l ba', ' bat', 'batt', 'atte', 'tter', 'tery', 'ery ', 'ry p', 'y po', ' pow', 'powe', 'ower', 'were', 'ered']\n"
     ]
    }
   ],
   "source": [
    "# Creating list of searches, ngrams and labels\n",
    "\n",
    "num_of_ngrams = []\n",
    "search_list = []\n",
    "n_grams = []\n",
    "labels = []\n",
    "shuffled_df = shuffle(joined)\n",
    "\n",
    "for i, row in shuffled_df.iterrows():\n",
    "    search_term =row['search_term']\n",
    "    ngrammed = word2ngrams(row['search_term'])\n",
    "    \n",
    "    if(i==1):\n",
    "        print('Example of search query:',search_term)\n",
    "        print('Example of ngrams: ',ngrammed)\n",
    "    if (len(search_term) > 0):\n",
    "        n_grams.append(ngrammed)\n",
    "        num_of_ngrams.append(len(ngrammed))\n",
    "        search_list.append(search_term)\n",
    "        labels.append(row['cat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe saver clamp\n",
      "(1, 1210)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Search list vectorization via tf-idf\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(search_list)\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([search_list[0]])\n",
    "# summarize encoded vector\n",
    "print(search_list[0])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3192\n",
      "Test size: 799\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing split\n",
    "\n",
    "M = vectorizer.transform(search_list).toarray()\n",
    "train_len = int(len(labels) * 0.8)\n",
    "X_train_tf = M[:train_len]\n",
    "search_list_train = search_list[:train_len]\n",
    "n_grams_train = n_grams[:train_len]\n",
    "y_train_tf = labels[:train_len]\n",
    "X_test_tf = M[train_len:]\n",
    "search_list_test = search_list[train_len:]\n",
    "n_grams_test = n_grams[train_len:]\n",
    "y_test_tf = labels[train_len:]\n",
    "print('Train size:',len(X_train_tf))\n",
    "# print(len(y_train_tf))\n",
    "print('Test size:',len(X_test_tf))\n",
    "# print(len(y_test_tf))\n",
    "print(X_train_tf[0])\n",
    "print(y_train_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to convert the data in a fasttext input format\n",
    "\n",
    "fasttext = False\n",
    "if (fasttext):\n",
    "    lab = '__label__'\n",
    "    joined['labs'] = lab + joined['cat'].astype(str)\n",
    "\n",
    "    # msk = np.random.rand(len(joined)) < 0.9\n",
    "    train = pd.DataFrame({'search_term':search_list_train,'labs':y_train_tf})\n",
    "    train['labs'] = lab + joined['cat'].astype(str)\n",
    "    test = pd.DataFrame({'search_term':search_list_test,'labs':y_test_tf})\n",
    "    test['labs'] = lab + joined['cat'].astype(str)\n",
    "    train[['search_term','labs']].to_csv('data/training_ft.tsv', sep='\\t',index=False,header=False)\n",
    "    test[['search_term','labs']].to_csv('data/test_ft.tsv', sep='\\t',index=False,header=False)\n",
    "\n",
    "    # ngrams\n",
    "    n_grams_train_flattened =[]\n",
    "    for ns in n_grams_train:\n",
    "        n_grams_train_flattened.append(' '.join(ns))\n",
    "    n_grams_test_flattened =[]\n",
    "    for ns in n_grams_test:\n",
    "        n_grams_test_flattened.append(' '.join(ns))\n",
    "\n",
    "    train_ngrams = pd.DataFrame({'ngrams':n_grams_train_flattened,'labs':y_train_tf})\n",
    "    train_ngrams['labs'] = lab + joined['cat'].astype(str)\n",
    "    test_ngrams = pd.DataFrame({'ngrams':n_grams_test_flattened,'labs':y_test_tf})\n",
    "    test_ngrams['labs'] = lab + joined['cat'].astype(str)\n",
    "    train_ngrams[['ngrams','labs']].to_csv('data/training_ngrams_ft.tsv', sep='\\t',index=False,header=False)\n",
    "    test_ngrams[['ngrams','labs']].to_csv('data/test_ngrams_ft.tsv', sep='\\t',index=False,header=False)\n",
    "\n",
    "    print(len(train))\n",
    "    print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC Accuracy: 85.48 %\n",
      "NB Accuracy: 82.23 %\n",
      "RF Accuracy: 84.61 %\n"
     ]
    }
   ],
   "source": [
    "# Fit the data in 3 different models and compare results:\n",
    "# 1-SVM\n",
    "# 2-NB\n",
    "# 3-RF\n",
    "\n",
    "\n",
    "# print(filtered_labels_train[0])\n",
    "clf = SGDClassifier().fit(X=X_train_tf,y=y_train_tf)\n",
    "predicted_sgdc = clf.predict(X_test_tf)\n",
    "# print(predicted_sgdc)\n",
    "print('SGDC Accuracy: %.2f %%'%round(np.mean(predicted_sgdc == y_test_tf)*100,2))\n",
    "\n",
    "nb = MultinomialNB().fit(X_train_tf, y_train_tf)\n",
    "predicted = nb.predict(X_test_tf)\n",
    "# print(predicted)\n",
    "print('NB Accuracy: %.2f %%'%round(np.mean(predicted == y_test_tf)*100,2))\n",
    "\n",
    "# print(filtered_labels_train[0])\n",
    "# clf = RandomForestClassifier(n_estimators=500,class_weight='balanced').fit(X=X_train_tf,y=y_train_tf)\n",
    "clf = RandomForestClassifier(n_estimators=500).fit(X=X_train_tf,y=y_train_tf)\n",
    "random_pred= clf.predict(X_test_tf)\n",
    "# print(predicted_sgdc)\n",
    "print('RF Accuracy: %.2f %%'%round(np.mean(random_pred== y_test_tf)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained GloVe vectors\n",
    "\n",
    "gloveFile = 'data/glove.6B/glove.6B.50d.txt'\n",
    "filename = 'data/glove.6B.100d.txt.word2vec'\n",
    "model_gensim = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "f = open(gloveFile,'r')\n",
    "model = {}\n",
    "for line in f:\n",
    "    splitLine = line.split()\n",
    "    word = splitLine[0]\n",
    "    embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "    model[word] = embedding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8523603677749634)]\n",
      "**********\n",
      "[('r.e.m.', 0.8340362906455994)]\n"
     ]
    }
   ],
   "source": [
    "# king - man + woman\n",
    "\n",
    "result = model_gensim.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "print('*'*10)\n",
    "\n",
    "chosen_word = 'radiohead'\n",
    "print(model_gensim.most_similar(chosen_word)[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sodpstone\n",
      "bateries\n",
      "ridgid multitool\n",
      "aspiradora\n",
      "roybi l18v\n",
      "roybi l18v\n",
      "handtools\n",
      "handtools\n",
      "rebarbender\n",
      "sawall\n",
      "drumel\n",
      "roybi l18v\n",
      "rebarbender\n",
      "come-along\n",
      "handtools\n",
      "taladros\n",
      "roybi l18v\n",
      "taladros\n",
      "respine\n",
      "roybi l18v\n",
      "ni-2.4v\n",
      "upholstry\n",
      "roybi l18v\n",
      "upholstry\n",
      "drumel\n",
      "bernzomatic\n",
      "carrrs\n",
      "roybi l18v\n",
      "hagchet\n",
      "drils\n",
      "tji\n",
      "hagchet\n",
      "drils\n",
      "roybi l18v\n",
      "taladros\n",
      "sawall\n",
      "inclinometer\n",
      "sandpap\n",
      "roybi l18v\n",
      "insallation\n",
      "susbenders\n",
      "drils\n",
      "sawall\n",
      "drils\n",
      "bernzomatic\n",
      "handtools\n",
      "bernzomatic\n",
      "roybi l18v\n",
      "handtools\n",
      "handtools\n",
      "bernzomatic\n",
      "bateries\n",
      "drils\n",
      "taladros\n",
      "handtools\n",
      "bateries\n",
      "ni-2.4v\n",
      "tji\n",
      "handtools\n",
      "handtools\n",
      "phillits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# average GloVe \n",
    "\n",
    "vecs_train = [] \n",
    "labs_train = []\n",
    "for s,lb in zip(search_list[:train_len],labels[:train_len]):\n",
    "    t = []\n",
    "    pnt = 0\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            t.append(model[w.lower()])\n",
    "             \n",
    "        except KeyError as e:\n",
    "            continue\n",
    "    try:\n",
    "        len(np.average(t,axis=0))\n",
    "        vecs_train.append(np.average(t,axis=0))\n",
    "        labs_train.append(lb)        \n",
    "    except:\n",
    "        print(s) \n",
    "\n",
    "# Training and Testing split\n",
    "\n",
    "X_train = np.array(vecs_train)\n",
    "y_train = np.array(labs_train)\n",
    "\n",
    "\n",
    "vecs_test = [] \n",
    "labs_test = []\n",
    "for s,lb in zip(search_list[train_len:],labels[train_len:]):\n",
    "    t = []\n",
    "    pnt = 0\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            t.append(model[w.lower()])\n",
    "             \n",
    "        except KeyError as e:\n",
    "            continue\n",
    "    try:\n",
    "        len(np.average(t,axis=0))\n",
    "        vecs_test.append(np.average(t,axis=0))\n",
    "        labs_test.append(lb)        \n",
    "    except:\n",
    "        print(s) \n",
    "\n",
    "X_test = np.array(vecs_test)\n",
    "y_test = np.array(labs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC Accuracy: 56.01 %\n",
      "NB Accuracy: 64.71 %\n",
      "RF Accuracy: 83.76 %\n"
     ]
    }
   ],
   "source": [
    "# Same models with GloVe\n",
    "\n",
    "clf = SGDClassifier().fit(X=X_train,y=y_train)\n",
    "predicted_sgdc = clf.predict(X_test)\n",
    "# print(predicted_sgdc)\n",
    "print('SGDC Accuracy: %.2f %%'%round(np.mean(predicted_sgdc == y_test)*100,2))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB().fit(X_train_scaled, y_train)\n",
    "predicted_nb = nb.predict(X_test_scaled)\n",
    "print('NB Accuracy: %.2f %%'%round(np.mean(predicted_nb == y_test)*100,2))\n",
    "\n",
    "\n",
    "# print(filtered_labels_train[0])\n",
    "# clf = RandomForestClassifier(n_estimators=500,class_weight='balanced').fit(X=X_train,y=y_train)\n",
    "clf = RandomForestClassifier(n_estimators=500).fit(X=X_train,y=y_train)\n",
    "random_pred= clf.predict(X_test)\n",
    "print('RF Accuracy: %.2f %%'%round(np.mean(random_pred== y_test)*100,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 10, 50)\n",
      "(3991,)\n",
      "(3991, 12)\n"
     ]
    }
   ],
   "source": [
    "# reshape data in 3 dimensions (len, 10, 50)\n",
    "\n",
    "vecs_nn = [] \n",
    "labs_nn = []\n",
    "empty = np.zeros(50)\n",
    "max_len = 10\n",
    "for s,lb in zip(search_list,labels):\n",
    "    t = []\n",
    "    pnt = 0\n",
    "    for w in s.split():\n",
    "        try:\n",
    "            #print(w)\n",
    "            t.append(model[w.lower()])\n",
    "            #vec = model[w]\n",
    "             \n",
    "        except KeyError as e:\n",
    "#            print(str(e) + ' not found')\n",
    "            continue\n",
    "    for f in range(0,max_len - len(t)):\n",
    "        t.append(empty)\n",
    "#    print(t)\n",
    "    vecs_nn.append(np.array(t))\n",
    "    labs_nn.append(lb)\n",
    "\n",
    "X = np.array(vecs_nn)\n",
    "print(X.shape)\n",
    "Y = np.array(labs_nn)\n",
    "print(Y.shape)\n",
    "num_categories = len(np.unique(labs_nn))\n",
    "Y_labs = keras.utils.to_categorical(Y,num_classes = num_categories)\n",
    "print(Y_labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 4, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 3, 4)           84        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "=================================================================\n",
      "Total params: 60,592\n",
      "Trainable params: 60,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3192 samples, validate on 799 samples\n",
      "Epoch 1/10\n",
      "3192/3192 [==============================] - 2s 736us/step - loss: 1.5039 - acc: 0.6689 - val_loss: 1.2414 - val_acc: 0.6433\n",
      "Epoch 2/10\n",
      "3192/3192 [==============================] - 1s 381us/step - loss: 1.1211 - acc: 0.6770 - val_loss: 1.0776 - val_acc: 0.6433\n",
      "Epoch 3/10\n",
      "3192/3192 [==============================] - 1s 399us/step - loss: 0.9661 - acc: 0.6908 - val_loss: 0.9398 - val_acc: 0.6783\n",
      "Epoch 4/10\n",
      "3192/3192 [==============================] - 1s 403us/step - loss: 0.8623 - acc: 0.7240 - val_loss: 0.9324 - val_acc: 0.6871\n",
      "Epoch 5/10\n",
      "3192/3192 [==============================] - 1s 402us/step - loss: 0.7872 - acc: 0.7503 - val_loss: 0.7928 - val_acc: 0.7597\n",
      "Epoch 6/10\n",
      "3192/3192 [==============================] - 1s 402us/step - loss: 0.7328 - acc: 0.7782 - val_loss: 0.7643 - val_acc: 0.7660\n",
      "Epoch 7/10\n",
      "3192/3192 [==============================] - 1s 412us/step - loss: 0.6847 - acc: 0.7954 - val_loss: 0.7402 - val_acc: 0.7710\n",
      "Epoch 8/10\n",
      "3192/3192 [==============================] - 1s 422us/step - loss: 0.6528 - acc: 0.8092 - val_loss: 0.7844 - val_acc: 0.7534\n",
      "Epoch 9/10\n",
      "3192/3192 [==============================] - 1s 448us/step - loss: 0.6218 - acc: 0.8098 - val_loss: 0.7019 - val_acc: 0.7885\n",
      "Epoch 10/10\n",
      "3192/3192 [==============================] - 1s 427us/step - loss: 0.5878 - acc: 0.8211 - val_loss: 0.6725 - val_acc: 0.7922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2b22ae80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM + CNN\n",
    "\n",
    "def create_lstm_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(InputLayer(input_shape=(10,50)))\n",
    "    model_conv.add(LSTM(100,dropout=0.1))\n",
    "    print()\n",
    "    model_conv.add(Reshape((5,4,5,)))\n",
    "    model_conv.add(Conv2D(4, 2, activation='relu'))\n",
    "    model_conv.add(MaxPooling2D(pool_size=2))    \n",
    "#    model_conv.add(Dense(num_categories, activation='relu'))\n",
    "    model_conv.add(Flatten())\n",
    "    model_conv.add(Dense(num_categories, activation='softmax'))\n",
    "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "                   \n",
    "model_conv = create_lstm_conv_model()\n",
    "print(model_conv.summary())\n",
    "model_conv.fit(X, Y_labs, validation_split=0.2, epochs = 10,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10, 500)           25500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 500)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 6, 100)            250100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 357,212\n",
      "Trainable params: 357,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3192 samples, validate on 799 samples\n",
      "Epoch 1/10\n",
      "3192/3192 [==============================] - 3s 933us/step - loss: 1.1752 - acc: 0.6773 - val_loss: 0.9831 - val_acc: 0.6721\n",
      "Epoch 2/10\n",
      "3192/3192 [==============================] - 2s 496us/step - loss: 0.8131 - acc: 0.7534 - val_loss: 0.7555 - val_acc: 0.7672\n",
      "Epoch 3/10\n",
      "3192/3192 [==============================] - 1s 460us/step - loss: 0.6388 - acc: 0.8127 - val_loss: 0.6755 - val_acc: 0.7860\n",
      "Epoch 4/10\n",
      "3192/3192 [==============================] - 2s 541us/step - loss: 0.5480 - acc: 0.8387 - val_loss: 0.6385 - val_acc: 0.8035\n",
      "Epoch 5/10\n",
      "3192/3192 [==============================] - 1s 457us/step - loss: 0.5043 - acc: 0.8465 - val_loss: 0.5999 - val_acc: 0.8198\n",
      "Epoch 6/10\n",
      "3192/3192 [==============================] - 1s 457us/step - loss: 0.4359 - acc: 0.8712 - val_loss: 0.5967 - val_acc: 0.8198\n",
      "Epoch 7/10\n",
      "3192/3192 [==============================] - 1s 433us/step - loss: 0.4047 - acc: 0.8772 - val_loss: 0.5801 - val_acc: 0.8235\n",
      "Epoch 8/10\n",
      "3192/3192 [==============================] - 1s 423us/step - loss: 0.3777 - acc: 0.8835 - val_loss: 0.5586 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "3192/3192 [==============================] - 1s 417us/step - loss: 0.3568 - acc: 0.8891 - val_loss: 0.5732 - val_acc: 0.8148\n",
      "Epoch 10/10\n",
      "3192/3192 [==============================] - 1s 441us/step - loss: 0.3399 - acc: 0.8894 - val_loss: 0.5638 - val_acc: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4d575cc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN + LSTM\n",
    "\n",
    "def create_conv_lstm_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Dense(500, activation='relu', input_shape=(10,50)))                \n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(100, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=5))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(num_categories, activation='softmax'))\n",
    "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "model_conv = create_conv_lstm_model()\n",
    "print(model_conv.summary())\n",
    "model_conv.fit(X, Y_labs, validation_split=0.2, epochs = 10,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1, 1210)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               524400    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 525,612\n",
      "Trainable params: 525,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2553 samples, validate on 639 samples\n",
      "Epoch 1/10\n",
      "2553/2553 [==============================] - 2s 878us/step - loss: 2.1886 - acc: 0.6420 - val_loss: 1.6760 - val_acc: 0.7011\n",
      "Epoch 2/10\n",
      "2553/2553 [==============================] - 1s 413us/step - loss: 1.2442 - acc: 0.6706 - val_loss: 0.9501 - val_acc: 0.7011\n",
      "Epoch 3/10\n",
      "2553/2553 [==============================] - 1s 406us/step - loss: 0.9119 - acc: 0.6718 - val_loss: 0.8357 - val_acc: 0.7042\n",
      "Epoch 4/10\n",
      "2553/2553 [==============================] - 1s 400us/step - loss: 0.7966 - acc: 0.7066 - val_loss: 0.7539 - val_acc: 0.7590\n",
      "Epoch 5/10\n",
      "2553/2553 [==============================] - 1s 401us/step - loss: 0.6857 - acc: 0.7830 - val_loss: 0.6835 - val_acc: 0.7872\n",
      "Epoch 6/10\n",
      "2553/2553 [==============================] - 1s 413us/step - loss: 0.5911 - acc: 0.8288 - val_loss: 0.6309 - val_acc: 0.8138\n",
      "Epoch 7/10\n",
      "2553/2553 [==============================] - 1s 396us/step - loss: 0.5201 - acc: 0.8570 - val_loss: 0.5925 - val_acc: 0.8341\n",
      "Epoch 8/10\n",
      "2553/2553 [==============================] - 1s 416us/step - loss: 0.4610 - acc: 0.8711 - val_loss: 0.5688 - val_acc: 0.8279\n",
      "Epoch 9/10\n",
      "2553/2553 [==============================] - 1s 402us/step - loss: 0.4301 - acc: 0.8747 - val_loss: 0.5510 - val_acc: 0.8279\n",
      "Epoch 10/10\n",
      "2553/2553 [==============================] - 1s 395us/step - loss: 0.3967 - acc: 0.8821 - val_loss: 0.5403 - val_acc: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a42ca59e8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT USING PRETRAINED VECTORS LSTM\n",
    "\n",
    "Y_labs_tf = np.array(y_train_tf)\n",
    "Y_labs_onehot_tf = keras.utils.np_utils.to_categorical(Y_labs_tf)\n",
    "input_shape_c = X_train[0].shape\n",
    "\n",
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(InputLayer(input_shape=(1,1210)))\n",
    "    \n",
    "    model_conv.add(LSTM(100,dropout=0.1))\n",
    "    \n",
    "    model_conv.add(Dense(num_categories, activation='softmax'))\n",
    "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "                   \n",
    "model_conv = create_conv_model()\n",
    "print(model_conv.summary())\n",
    "model_conv.fit(X_train_tf.reshape(len(X_train_tf),1,1210),Y_labs_onehot_tf, validation_split=0.2, epochs = 10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1, 500)            605500    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 50, 10)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 10)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 49, 1000)          21000     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 24, 1000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               440400    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 1,068,112\n",
      "Trainable params: 1,068,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2553 samples, validate on 639 samples\n",
      "Epoch 1/10\n",
      "2553/2553 [==============================] - 9s 4ms/step - loss: 1.1964 - acc: 0.6765 - val_loss: 0.8058 - val_acc: 0.7308\n",
      "Epoch 2/10\n",
      "2553/2553 [==============================] - 7s 3ms/step - loss: 0.6864 - acc: 0.7979 - val_loss: 0.6511 - val_acc: 0.8013\n",
      "Epoch 3/10\n",
      "2553/2553 [==============================] - 7s 3ms/step - loss: 0.5135 - acc: 0.8582 - val_loss: 0.6706 - val_acc: 0.8153\n",
      "Epoch 4/10\n",
      "2553/2553 [==============================] - 7s 3ms/step - loss: 0.4275 - acc: 0.8754 - val_loss: 0.6510 - val_acc: 0.8138\n",
      "Epoch 5/10\n",
      "2553/2553 [==============================] - 7s 3ms/step - loss: 0.3907 - acc: 0.8876 - val_loss: 0.6109 - val_acc: 0.8310\n",
      "Epoch 6/10\n",
      "2553/2553 [==============================] - 8s 3ms/step - loss: 0.3400 - acc: 0.8966 - val_loss: 0.6062 - val_acc: 0.8279\n",
      "Epoch 7/10\n",
      "2553/2553 [==============================] - 8s 3ms/step - loss: 0.3119 - acc: 0.9033 - val_loss: 0.6558 - val_acc: 0.8279\n",
      "Epoch 8/10\n",
      "2553/2553 [==============================] - 8s 3ms/step - loss: 0.2872 - acc: 0.9068 - val_loss: 0.6188 - val_acc: 0.8263\n",
      "Epoch 9/10\n",
      "2553/2553 [==============================] - 8s 3ms/step - loss: 0.2809 - acc: 0.9095 - val_loss: 0.6118 - val_acc: 0.8388\n",
      "Epoch 10/10\n",
      "2553/2553 [==============================] - 7s 3ms/step - loss: 0.2607 - acc: 0.9056 - val_loss: 0.6183 - val_acc: 0.8466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a432f5ba8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT USING PRETRAINED VECTORS CNN-LSTM\n",
    "\n",
    "Y_labs_tf = np.array(y_train_tf)\n",
    "Y_labs_onehot_tf = keras.utils.np_utils.to_categorical(Y_labs_tf)\n",
    "input_shape_c = X_train[0].shape\n",
    "\n",
    "def create_conv_lstm_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Dense(500, activation='relu', input_shape=(1,1210)))\n",
    "    model_conv.add(Reshape((50, 10)))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(1000, 2, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=2))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(num_categories, activation='softmax'))\n",
    "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "model_conv = create_conv_lstm_model()\n",
    "print(model_conv.summary())\n",
    "model_conv.fit(X_train_tf.reshape(len(X_train_tf),1,1210),Y_labs_onehot_tf, validation_split=0.2, epochs = 10,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
